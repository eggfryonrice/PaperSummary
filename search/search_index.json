{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udcda Paper Summaries","text":"<p>This is collection of my summarized research papers.</p> <ul> <li> <p>Trustworthy AI: From Principles to Practices</p> </li> <li> <p>Reconciling modern machine learning practice and the bias-variance trade-off</p> </li> </ul>"},{"location":"#math-test","title":"Math Test","text":"<p>Here is inline math: $E = mc^2$</p>"},{"location":"Belkin2019/","title":"Reconciling modern machine learning practice and the bias-variance trade-o\ufb00","text":"<p>I am reading this paper to see how bias-variance tradeoff I learned in CS376 can be applied to DNN, which has different properties compared to classical models.</p>"},{"location":"Belkin2019/#introduction","title":"Introduction","text":"<p>Machine learning on the problem of prediction does the following: given training examples from $\\mathbb{R}^d \\times \\mathbb{R}$, we learn predictor $h_n: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ by choosing from some function class $\\mathcal{H}$, which is neural network with certain architecture, by minimizing empirical risk $\\frac{1}{n} \\sum_{i=1}^{n} \\ell(h(x_i), y_i)$</p> <p>The challenge here is mismatch between minimzaing empirical risk and minimizing true (or test) risk $\\mathbb{E}_{(x,y)~P}[l(h(x), y)]$. Conventional wisdom in mathince learning controls the capacity of $\\mathcal{H}$ based on bias-variance tradeoff by finding the \"sweet spot\" between underfitting and overfitting. Control of function class can be explicit by choice of $\\mathcal{H}$, or be implicit using regularization. The performance of the model shows U-shaped risk curve.</p> <p>But in modern mathince learning, large neural networks with hight function class capacity and near-perfect fit to training data often give very accurate predictions on new data. Recent empirical evidence indicates taht neural networks and kernel mathinces trained to interpolate the training data obtain near optimal test results even when the training data are corrupted with high levels of noise.</p> <p>The main finding of this paper is a pattern, \"double descent\" risk curve, which is pattern of how performance on unseen data depends on model capacity, and the meahcanism underlying its emergence. When funtion class capacity is below interpolation threshold, learend predictors exhibit the classical U-shaped curve. But increasing the function class capacity beyond interpolation threshold leads to decreasing risk, typically going below the risk achieved at the sweet point in classical regime. Learned predictors on the right of the interpolation threshold fit the training data perfectly and have zero empirical risk, but those frome richer function classes tends to smaller norm and are thus simpler. </p>"},{"location":"Belkin2019/#neural-networks","title":"Neural networks","text":""},{"location":"Belkin2019/#random-fourier-features","title":"Random Fourier features","text":"<p>The paper first consider popular class of nonlinear parametric models called Random Fourier Features (RFF). It can be vieewd as a class of two layer neural networks with fixed weights in the first layer. RFF model family $\\mathcal{H}N$ with $N$ parameters consists of fucntion $h: \\mathbb{R}^2 -&gt; \\mathbb{C}$ of the form $$ h(x) = \\sum{k=1}^{N} a_k \\, \\phi(x; v_k) \\quad \\text{where} \\quad \\phi(x; v) := e^{\\sqrt{-1} \\langle v, x \\rangle}, $$ and $v_1, \\dots , v_N$ are sampled independently from normal distribution. We consider $\\mathcal{H}_N$ as class of real valued function as class of real valued function with $2N$ real valued parameters (real and imaginary). </p> <p>(As $N\\rightarrow \\infty$, $\\mathcal{H}N$ becomes close aproximation to Reproducing Kernel Hilbert Space (RKHS) corresponding to gaussian kernel, denoted by $\\mathcal{H}{\\infty}$.)</p> <p>Given $n$ datas from $\\mathbb{R}^d \\times \\mathbb{R}$, they find $h_{n, N} \\in \\mathcal{H}_N$ by ERM with squared loss. But minimizer is not unique for $N &gt; n$. In that case, they choose minimizer with the minimum $L_2$ loss coefficients. </p> <p>When $n &lt; N$, we can observe classical U-shaped curve that can be predicted by bias variance tradeoff. for $n&gt;N$, where all function classes are rich enough to achieve zero training risk, increasing N progressively construct better approximation, showing the second descent segment of the curve. </p> <p>There are more experiments with other dataset and models structure in appendix.</p>"},{"location":"Belkin2019/#neural-newtorks-and-backpropagation","title":"Neural newtorks and backpropagation","text":"<p>In general multilayer neural networks, we use stochastic gradient descent (SGD) to fit the trainin data. They observed that increasing the number of parameters in fully connected two layer neural networks lead to a risk curve qualitatively similar to that observed with RFF models. </p> <p>The compuational complexity of ERM with neural networks make double descent risk curve difficult to observe. In under parametrized regime, the result is highly sensitive to initialization due to nonconvexity of the ERM optimization. This variability in training and test risks masks the double descent curve. </p> <p>To have this effect for datasets as large as ImageNet, model size should have extensive amout of parameters. The amount of parameters needed is larget than many neural network models for ImageNet. In such cases, classical regime of the U-shaped risk curve is more appropriate to understand generalization. For smaller datasets, simply training to obtain zero training risk often results in good test performance.</p>"},{"location":"Belkin2019/#decision-trees-and-ensemble-methods","title":"Decision trees and ensemble methods","text":"<p>They experimented with random forest. To further enlarge the function class, they also considered ensembles of several interpolating trees. As a result, double descent curve appear just as with neural networks.</p>"},{"location":"Belkin2019/#concluding-thoughts","title":"Concluding thoughts","text":"<p>For random Fourier or random ReLU features, solutions are constructed explicitly by minimum norm linear regression in the feature space. For neural networks, we used SGD. SGD initialized at zero converges to minimum norm solution. While SGD for more general case is not fully understood, there is some evidence that similar minimum norm inductive bias is present. Ensembling in decision tree leads to interpolating solution with higher degree of smoothness, and those averaged solution performed better than any individual tree. So all three methods lead to minimum norm solution. </p> <p>In this paper, modern models usually outperform the classical model on the test set. But beside that, there is a growing understanding that larger models are easy to optimize as local methods, such as SGD, converge to global minima of the training risk in over parametrized regimes. Thus, large interpolting models have low test risk and easy to optimize at the same time. It is likely that the models to the left of the interpolation peak have optimzation properties qualitatively different from those to the right.</p> <p>The understanding of model performance developed in this work delineates the limits of classical analyses and opens new lines of enquiry to study and compare computational, statistical, and mathematical properties of the classical and modern regimes in machine learning.</p>"},{"location":"Li2023/","title":"Trustworthy AI: From Principles to Practices","text":"<p>I am reading this survey to overview what kind of research is going on on field of trustworthy AI. </p>"},{"location":"Li2023/#introduction","title":"Introduction","text":"<p>AI practitioners traditionally considered system performance to be the main metric in their workflows. But other kinds of aspect should also be considered to improve trustworthiness, such as robustness, algorithmic fairness, explainability, and transparency. Those aspects should be consider throughout the while lifecycle of an AI system, and those aspect may interfere with each other. </p>"},{"location":"Li2023/#aspects-beyond-predictive-accuracy","title":"Aspects beyond predictive accuracy","text":""},{"location":"Li2023/#robustness","title":"Robustness","text":"<p>AI can be vulnerable at the level of data, algorithms, and systems, respectively.  If AI model is trained without considering the diverse distributions of data in different secenarios, its performance will  e significantly affected. The robustness against distributional shift has been a common problem. For example, in the field of autonomous driving, industry are working hard to enhance the performance of vehicles in nightime or rainy scenes to guarantee the system's reliability. </p> <p>Adversarial attack and defenses against it have raised concerns in both academia and the industry in recent years. Some study cateraorized adversarial attack with respect to the attack timing. Decision time attack perturbs the input samples, training time attack injects carefully desinged samples into training data. There are other type of attacks such as feature space attacks, problem space attacks, and model stealing.</p> <p>System level robustness against illegal inputs should alsobe considered. For example, lidar perceptron system for autonomous vehicle might perceive laser beams emitted by lidars in other vehicles.</p> <p>Proper test is needed to evaluate and enhance to robustness of AI. Threre are functional test and performance test. From the attackers's perspective, the rate of success of an attack intuitively measures the robustness of the system.</p> <p>We may also consider certified verification of the adversarial robustness of an AI model. For example, we may derive lower bound of the inimum distortion to an attack on an AI model.</p>"},{"location":"Li2023/#generalization","title":"Generalization","text":"<p>Genearalization is she capability to make accurate predictions regarding unseen data. Generalization has an impact on AI trustworthiness. AI should make predictions on realistic data, even on domains or distributions which they are not trained, so generalization affects the reliability and risk of practical systems. Also, AI models should be able to generlaize without exhaustively collect and annotate large amounts of data for various domains, so deployment of AI system can be more affordable and sustainable.</p> <p>Generalization is closely related to robustness. An algorithm that is robust agains small perturbations has better generalization, but robustness agains different data distributions may hurt the model's generalization.</p> <p>To evaluate genearlization, past ML studies have developed rich approaches to measure the bounds of models' generalization error, such as Rademacher complexity and Vapnik-Chervonenkis dimension. Deep Neural Network (DNN) has property that it obtains generalization despite their massive capcity, that has been examined by the perspective of bias-variance tradeoff.</p>"},{"location":"Li2023/#explainability-and-transparency","title":"Explainability and Transparency","text":"<p>There is a demand for AI service users for the right to know the intention, business model, and technological mechanism of AI products.</p> <p>Explainability is understanding how an AI model makes its decision. From the perspective of scientific research, we need understanding about how data, parameters, precedures, and outcomes of AI. From persepective of building AI models, we need explainability to better use AI. </p> <p>Some studies tries to design explainable model. There are series of fully or partially explainable ML models. However, some complex models like DNN have exhibited better performance, it can't be explained by their design. Researcheres does post hoc explanation, that addresses model's behavior by analyzing its input, intermediate result, and output. </p> <p>Since there is ambiguity of the psychological outlining of explainability, unified evaluation of explainability has been recognized as a challenge. There are some approaches such as subjective human evaluation and human-Ai task performacne.  Despite these evaluations, directy quantitative measurement of explainability remains a problem. </p> <p>Transparency considers Ai as a software system, and seeks to disclose information regarding it sentire lifecycle. Transparency serves as basic requirement to build public's trust in Ai systems.</p> <p>To make AI system transparent, variety of information regarding to its creation should be sisclosed. The recent trend of open source systems significantly contributes to the algorithmic transparency. Transparency of the runtime process and decision making should also be considered in various scenarios. Qualitative evaluation of transparency has undergone recent advances in the AI industry. </p>"},{"location":"Li2023/#reproducibility","title":"Reproducibility","text":"<p>Reproducibility enables effective verificaiton of research, and allos the community to quickly convert the lastest approaches into practice or conduct follow-up research. </p>"},{"location":"Li2023/#fairness","title":"Fairness","text":"<p>It is important for practitioners to keep that the fairness of AI systems in mind to avoid instilling or exacerbating social bias.</p> <p>Common objective of fairness is to mitigate the effects of biases. Bias often manifest in the form of unfair treatment of different groups of people based on their protected inforation. </p> <p>Fairness can be applicable at multiple granularities of system behavior. At each granularity, we concern fairness of outcome and fairness of process. In tasks like face identification, we concern the aggregated behavior and the bias among different groups. In task like resume reviewing for candidate screening, wheresensitive variable can be easility decoupled from the other features that determine the system's prediction, we consider bias among individuals.</p> <p>In the group evel, there can be different types of fairness. Independence requires system to be statistically independent of sensetive variables. Seperation requires that independence principle hold conditioned on the unerlying ground truth. Sufficiency true outcome and sensitive variable to be independent. These principles are mutually exclusive under certain circumstances.</p> <p>Metrices of fairness can differ according to the properties of model and tasks. This can depend on  - whether task output, sensitive variables, model prediction are discrete or continuous - whether there is enough empirical data - number of sensitive variables we have </p>"},{"location":"Li2023/#privacy-protection","title":"Privacy protection","text":"<p>Privacy protection refers to protecting against unauthorized use of the data that can directly or indirectly identify a person or household. There are techniques to protect the privacy in data collection and processing, model training, and model deployment. There are various mathematical methods to formally verify the protectiveness of privacy-preserving approaches. </p>"},{"location":"Li2023/#systematic-approach-for-trustworthy-ai","title":"Systematic approach for trustworthy AI","text":""}]}